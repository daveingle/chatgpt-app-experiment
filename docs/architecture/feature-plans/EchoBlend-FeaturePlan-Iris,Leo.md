<file name=0 path=EchoBlend-FeaturePlan-Iris.md># EchoBlend â€“ Feature Plan (RFC)  
*Author: IrisÂ Zhang â€“ Onâ€‘device AI/ML & Privacy*  
*Last updated: {{TODAY}}*

---

## 1Â Â Summary
EchoBlend transforms raw voice input into emotionallyâ€‘aware audio sprites and matching visual/haptic accents, entirely onâ€‘device.  
This RFC outlines the classifier integration, privacy scaffolding, and developerâ€‘facing API necessary to ship vâ€‘1 of EchoBlend without any cloud dependency and with strict dataâ€‘retention boundaries.

## 2Â Â Problem / Opportunity
Interactive replays feel lifeless when every recording is treated the same. Attaching a fast, lightweight emotion classifier lets us:
* **Adapt** music layers, haptic rhythms, and colour palettes in realâ€‘time.  
* **Personalise** replays without sending raw audio offâ€‘device.  
* **Delight** users with subtle changes that match their tone of voice.

## 3Â Â Goals
|Â IDÂ | Goal | SuccessÂ Metric |
|---|------|----------------|
|â€¯Gâ€‘1 | Classify 8 emotions onâ€‘device | â‰¥â€¯90â€¯% macroâ€‘F1 on balanced dataset |
|â€¯Gâ€‘2 | Subâ€‘10â€¯ms inference on A17; â‰¤â€¯25â€¯ms on A13 | Median latency (XCTest benchmark) |
|â€¯Gâ€‘3 | Zero rawâ€‘audio persistence | Static & runtime privacy audit passes |
|â€¯Gâ€‘4 | Public Swift API â‰¤â€¯5 lines to adopt | DX survey: â€œvery easyâ€ |

### Nonâ€‘Goals
* Cloud fallback or federation (future work).  
* Multilingual emotion models (English only for vâ€‘1).

## 4Â Â Architecture Overview
```mermaid
flowchart LR
  mic[ğŸ™ï¸Â Mic Capture] --> pre[Preâ€‘processing]
  pre --> cls[MLVoiceEmotionClassifierÂ ğŸ“¦]
  cls --> vec[8â€‘D Emotion Vector]
  vec --> eng[EchoBlendEngine]
  eng -->|trackÂ ID| audio[AudioÂ SpriteÂ Player]
  eng -->|IntensityProfile| haptic[HapticPaletteKit]
  eng -->|HSBA palette| chroma[ChromaPreviewView]
  subgraph Storage
    vault[(DataVaultÂ ğŸ”)]
  end
  pre -.no frames.-> vault
```

### 4.1Â Â Engineâ€¯&â€¯Audioâ€‘Feel Scopeâ€‚(LeoÂ Ramirez)

* **Layerâ€‘picker** â€“ Route the 8â€‘D emotion vector to perâ€‘track sample stacks (drums, pads, FX) via weighted roulette, keeping variety while emphasising the dominant emotion.  
* **Heartâ€‘rate tempo mapping** â€“ Ingest live BPM from HealthKit/AppleÂ Watch; apply linear mapping up toâ€¯160â€¯BPM, then switch to Â½â€‘time division above that threshold to avoid chipmunk artefacts. Expose `updateBPM(_:)`.  
* **Palette & haptic sync** â€“ On kick/fundamental transients, emit HSBA keyframes to `ChromaPreviewView` and intensity envelopes to `HapticPaletteKit`. Endâ€‘toâ€‘end latency budget â‰¤â€¯8â€¯ms.  
* **Chiptune easter egg** â€“ Detect a silent threeâ€‘beat tap (~400â€“600â€¯ms spacing) to swap current track layers with NSFâ€‘style 8â€‘bit samples; repeat gesture toggles back (accessibilityâ€‘friendly).  
* **Deterministic test harness** â€“ Record pseudoâ€‘random seeds plus fixture emotion/BPM streams to enable fully replayable unit tests.

* **Preâ€‘processing**Â â€“ 16Â kHz mono, 1Â s window Hanningâ€‘tapered.  
* **Classifier**Â â€“ pruned/quantised CoreÂ ML model (<â€¯2Â MB).  
* **EchoBlendEngine**Â â€“ pure Swift; exposes `updateEmotion(_:)`.  
* **DataVault**Â â€“ encrypted CoreÂ Data store for tiny perâ€‘user tuning blobs; never stores raw frames.

## 5Â Â Public API Surface (`EchoBlendKit`)
```swift
public struct EightDimEmotion: Sendable, Equatable {
  public var joy, sadness, anger, fear, disgust, surprise, neutral, custom: Float
}

public protocol EmotionConsumer {
  mutating func updateEmotion(_ vector: EightDimEmotion)
}

/// Irisâ€‘provided extension point
extension EchoBlendEngine: EmotionConsumer { /* impl. */ }
```

## 6Â Â Privacy & Security
* All inference happens inside an **AppÂ Privacy Manifest** â€œOnâ€‘device Onlyâ€ block.  
* `DataVault` key stored in SecureÂ Enclave (see ticketÂ #DVâ€‘1).  
* No network permission required in theâ€¯.entitlements for vâ€‘1.  

## 7Â Â Testing & Metrics
| Layer | Test | Tool |
|-------|------|------|
| Classifier | Accuracy & latency | `XCTest` + `XCMetrics` |
| Engine | Correct trackâ€‘selection mapping | Deterministic fixtures |
| Privacy | Static scan | `swiftâ€‘lintâ€‘privacy` custom rule |
| Storage | Encryption roundâ€‘trip | Unit test with inâ€‘mem store |

## 8Â Â Milestones
|Â WeekÂ | Deliverable | Owner |
|------|-------------|-------|
|Â Tâ€‘0Â | Pruned/quantised `.mlmodel` + unit benchmarks | Iris |
|Â Tâ€‘1Â | `updateEmotion(_:)` wired to EchoBlendEngine | IrisÂ &Â Leo |
|â€¯Tâ€‘1aâ€¯| Engine scope (layerâ€‘picker, BPM mapping, palette/haptic sync) | Leo |
|â€¯Tâ€‘1bâ€¯| Chiptune easter egg implementation & replayable test harness | Leo |
|Â Tâ€‘2Â | QuickLook preview provider (UTI from Lux) | Iris |
|Â Tâ€‘3Â | Endâ€‘toâ€‘end latency & privacy audit signâ€‘off | IrisÂ &Â Sage |

## 9Â Â Risks & Mitigations
* **Classifier bloat** blows past 2â€¯MB â†’ aggressive weightâ€‘sharing or oneâ€‘hot quant.  
* **A13 latency** outliers â†’ fallback heuristics (GMM) when >â€¯25â€¯ms.  
* **QuickLook UTI stall** â†’ ship without preview; CLI tool fallback for Lux.

## 10Â Â Open Questions
1. Final label taxonomy: keep â€œneutralâ€ or split into â€œneutralâ€‘positive / neutralâ€‘negativeâ€?  
2. QuickLook preview payload: PNG waveform vsâ€¯JSON colour stops?  
3. Should user be able to disable emotionâ€‘based adaption in settings?

---

*Please comment inline or approve by adding âœ…Â in the PR description.*
