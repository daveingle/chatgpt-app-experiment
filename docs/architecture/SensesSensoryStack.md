

# SENSESÂ SensoryÂ Stack

_A crossâ€‘layer map of how a single â€œmomentâ€ travels from capture to multiâ€‘modal playback across Apple platforms._

```mermaid
graph LR
  %% â”€â”€â”€â”€â”€ Capture Layer â”€â”€â”€â”€â”€
  subgraph CaptureÂ Layer
    A1[Text Entry âœï¸]:::cap
    A2[PhotoÂ /Â Video ðŸ“·]:::cap
    A3[Audio Snippet ðŸŽ™ï¸]:::cap
  end
  %% â”€â”€â”€â”€â”€ ML Processing â”€â”€â”€â”€â”€
  subgraph MLÂ Processing
    P1[EmotionClassifier.mlmodel]:::proc
    P2[ColorMoodNet.mlmodel]:::proc
    P3[VoiceSentimentNet.mlmodel]:::proc
  end
  %% â”€â”€â”€â”€â”€ Asset Generation â”€â”€â”€â”€â”€
  subgraph AssetÂ Generation
    G1[EchoBlendKitÂ ðŸŽ§]:::gen
    G2[HapticPaletteKitÂ ðŸ¤²]:::gen
    G3[OrbSceneBuilderÂ ðŸª©]:::gen
    G4[SmellLaterProfile.jsonÂ ðŸ§ª]:::gen
  end
  %% â”€â”€â”€â”€â”€ Crossâ€‘device Rendering â”€â”€â”€â”€â”€
  subgraph Crossâ€‘deviceÂ Rendering
    R1[iOS Postcard View ðŸ“±]:::rend
    R2[watchOS HapticÂ Player âŒšï¸]:::rend
    R3[visionOS SpatialÂ Orb ðŸ‘“]:::rend
    R4[BLEÂ Scent Diffuser ðŸŒ¸]:::rend
  end

  %% Flow edges
  A1 -- textÂ features --> P1
  A2 -- pixelÂ histogram --> P2
  A3 -- MFCCs --> P3

  P1 --> G1
  P1 --> G2
  P2 --> G2
  P2 --> G3
  P3 --> G1
  P3 --> G2

  G1 --> R1
  G1 --> R2
  G2 --> R1
  G2 --> R2
  G3 --> R3
  G4 --> R4

  %% Styling
  classDef cap  fill:#E0F7FA,stroke:#00796B,color:#004D40;
  classDef proc fill:#FFF3E0,stroke:#F57C00,color:#E65100;
  classDef gen  fill:#E8F5E9,stroke:#388E3C,color:#1B5E20;
  classDef rend fill:#E3F2FD,stroke:#1976D2,color:#0D47A1;
```

---

## LayerÂ Breakdown

| Layer | Purpose | Key Components | Notes |
|-------|---------|----------------|-------|
| **Capture** | Raw user input | Text, Photo/Video, Short Audio | All processed **onâ€‘device**; zero cloud storage. |
| **MLÂ Processing** | Lightweight sentiment + semantic inference | â€¢ `EmotionClassifier` (CoreÂ ML) Â â€¢ `ColorMoodNet` (VGGâ€‘lite) Â â€¢ `VoiceSentimentNet` (TinyConvâ€‘RNN) | TargetsÂ <â€¯10â€¯ms inference on iPhoneÂ 16; models bundled & update via inâ€‘app MLÂ patches. |
| **AssetÂ Generation** | Translate ML output into multiâ€‘modal assets | â€¢ **EchoBlendKit**Â (audio) Â â€¢ **HapticPaletteKit**Â (.ahap) Â â€¢ **OrbSceneBuilder**Â (SceneKit / RealityKit) Â â€¢ **SmellLaterProfile.json** | Modular; SwiftÂ packages under `Packages/Services/â€¦`. |
| **Crossâ€‘deviceÂ Rendering** | Deliver the â€œliving postcardâ€ | iOS (core view), watchOS (wrist haptics), visionOS (spatial orb), **BLEÂ Scent Diffuser** (future) | Renderer picks best asset subset per platform. |

---

### â€œSmellâ€¯Laterâ€Â BLE ExtensionÂ HookÂ (v0.1)

| Field | Type | Range | Units | Description |
|-------|------|-------|-------|-------------|
| **ServiceÂ UUID** | â€“ |Â 0xA11E | â€“ | Primary Scent Diffuser service |
| **TopNoteIntensity** |Â UInt8 |Â 0â€“100 |Â % | Relative topâ€‘note strength |
| **MidNoteIntensity** |Â UInt8 |Â 0â€“100 |Â % | Relative midâ€‘note strength |
| **BaseNoteIntensity** |Â UInt8 |Â 0â€“100 |Â % | Relative baseâ€‘note strength |
| **MixDuration** |Â UInt16 |Â 0â€“600 |Â seconds | How long the blend should persist |

**ExampleÂ payload**

```json
{
  "top": 75,
  "mid": 40,
  "base": 20,
  "duration": 90
}
```

> _Design rationale:_ keeping payload â‰¤Â 20â€¯bytes ensures it fits a single BLEÂ write without fragmentation on common diffusers.

---

### Privacy & Ethics Snapshot (1â€‘PagerÂ Outline)

1. **Onâ€‘device only** data path; no cloud relay by default.  
2. Max retention: **30Â days** of raw captures; assets may persist longer (userâ€‘optâ€‘in).  
3. All MLÂ models are compact & explainable; no largeâ€‘scale profiling.  
4. Accessibility sliders give neuroâ€‘divergent users control over sensory intensity.  

_Detailed sidebar to follow in a separate doc._
